{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install scikit-optimize\n","!git clone https://github.com/modestyachts/cifar-10.2.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G07vrg_DWKeW","executionInfo":{"status":"ok","timestamp":1673977678729,"user_tz":-60,"elapsed":27270,"user":{"displayName":"Eirik Grytøyr","userId":"09803395650685047322"}},"outputId":"0f4ed8a6-4f20-4eac-a455-e9b7bd0e690d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit-optimize\n","  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyaml>=16.9\n","  Downloading pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.8/dist-packages (from scikit-optimize) (1.0.2)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from scikit-optimize) (1.7.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-optimize) (1.2.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from scikit-optimize) (1.21.6)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.1.0)\n","Installing collected packages: pyaml, scikit-optimize\n","Successfully installed pyaml-21.10.1 scikit-optimize-0.9.0\n","Cloning into 'cifar-10.2'...\n","remote: Enumerating objects: 40, done.\u001b[K\n","remote: Total 40 (delta 0), reused 0 (delta 0), pack-reused 40\u001b[K\n","Unpacking objects: 100% (40/40), done.\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import random\n","import os\n","import numpy as np\n","from time import perf_counter\n","from google.colab import drive\n","from keras import optimizers\n","from keras.callbacks import EarlyStopping\n","from keras.models import Model, Sequential, load_model\n","from keras.backend import clear_session\n","from keras.layers import Dense, Dropout, Activation, BatchNormalization, Resizing, GlobalAveragePooling2D\n","from keras.optimizers import Adam\n","from keras.losses import CategoricalCrossentropy, KLDivergence \n","from keras.utils import to_categorical\n","from tensorflow.keras.applications import inception_v3, vgg16, ResNet50, resnet50  \n","import skopt\n","from skopt import gp_minimize\n","from skopt.space import Real, Integer, Categorical, LogN\n","from skopt.plots import plot_convergence\n","from skopt import callbacks\n","from skopt.callbacks import CheckpointSaver\n","\n","\n","drive.mount('/content/drive')\n","gdrive_dir = '/content/drive/MyDrive/Colab Notebooks/CI/Logfiles/'  # --change to your path--\n","assert os.path.exists(gdrive_dir)\n","\n","tf.random.set_seed(420)\n","tf.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"NZ_OvYhE8Kbr","executionInfo":{"status":"ok","timestamp":1673977707575,"user_tz":-60,"elapsed":28867,"user":{"displayName":"Eirik Grytøyr","userId":"09803395650685047322"}},"outputId":"eaa07ee8-59ac-41c1-a7c1-193f1ab94172"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["'2.9.2'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# Cifar 10.2 \n","def preprocessCIFAR10_2():\n","  train_data = np.load('cifar-10.2/cifar102_train.npz')\n","  test_data = np.load('cifar-10.2/cifar102_test.npz')\n","  label_names = test_data['label_names']\n","\n","  x_train = train_data['images']\n","  y_train = train_data['labels']\n","\n","  indexes_test = np.array(range(len(x_train)))\n","  random.shuffle(indexes_test)\n","  x_train = x_train[indexes_test]\n","  y_train1 = y_train[indexes_test]\n","\n","  x_test = test_data['images']\n","  y_test = test_data['labels']\n","\n","  # One-Hot encoding\n","  y_train = to_categorical(y_train1, num_classes=10)\n","  y_test = to_categorical(y_test, num_classes=10)\n","  return(x_train, y_train, x_test, y_test, label_names)"],"metadata":{"id":"8-MzMl64SYA0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_learning_curves(history):\n","  acc = history.history['accuracy']\n","  val_acc = history.history['val_accuracy']\n","\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","\n","  plt.figure(figsize=(8, 8))\n","  plt.subplot(2, 1, 1)\n","  plt.plot(acc, label='Training Accuracy')\n","  plt.plot(val_acc, label='Validation Accuracy')\n","  plt.legend(loc='lower right')\n","  plt.ylabel('Accuracy')\n","  plt.ylim([min(plt.ylim()),1])\n","  plt.title('Training and Validation Accuracy')\n","\n","  plt.subplot(2, 1, 2)\n","  plt.plot(loss, label='Training Loss')\n","  plt.plot(val_loss, label='Validation Loss')\n","  plt.legend(loc='upper right')\n","  plt.ylabel('Cross Entropy')\n","  plt.title('Training and Validation Loss')\n","  plt.xlabel('epoch')\n","  plt.show()"],"metadata":{"id":"Jf0iihfnYjO4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, classification_report\n","\n","def print_report(model, x, y, labels):\n","\n","  # Prediction\n","  predictions = model.predict(x)\n","  # Select for each observation the highest probability\n","  predictions = np.argmax(predictions, axis=1)\n","  real = np.argmax(y, axis=1)\n","  # Confusion matrix\n","  confusionMatrix = confusion_matrix(real, predictions)\n","  print(pd.DataFrame(confusionMatrix, columns=labels, index=labels))\n","\n","  # Classification Report\n","  print(classification_report(real, predictions, target_names=labels,))\n","  plot_confusion_matrix(confusionMatrix, \"Final Model Confusion Matrix\")\n"],"metadata":{"id":"nsw_uFqD-DaX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_model(name, nlayers, units, dropout_rate):\n","\n","  if name == 'resNet50':\n","    base_model = ResNet50\n","  elif name == 'inceptionv3':\n","    base_model = inception_v3.InceptionV3\n","  elif name == 'vgg16':\n","    base_model = vgg16.VGG16\n","\n","  # Defining the model\n","  base_model = base_model(\n","      include_top=False,\n","      weights='imagenet',\n","      input_shape = (299, 299, 3),\n","      classes=10\n","  )\n","\n","  # Resize + Base Model\n","  final_model = Sequential()\n","  final_model.add(Resizing(299, 299))\n","  final_model.add(base_model)\n","\n","  # Define new layers\n","  new_layers = Sequential()\n","  new_layers.add(GlobalAveragePooling2D(name='avg_pool'))\n","  for i in range(nlayers):\n","    u = units * 2 ** -i\n","    new_layers.add(Dense(u, activation='relu'))\n","    new_layers.add(Dropout(dropout_rate))\n","  new_layers.add(Dense(10, activation='softmax'))\n","\n","  # Joining both models\n","  final_model.add(new_layers)\n","  \n","  return final_model, base_model, new_layers"],"metadata":{"id":"dgGWl42nxmNy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(params, model_name, nlayers, reuse_params=None, eval=False, learning_curve=False):\n","  clear_session()\n","  t_start = perf_counter()\n","\n","  epochs1 = 100\n","  epochs2 = 100\n","  patience = 5\n","\n","  if reuse_params:\n","    units, dropout = params\n","    lr, batch_size, loss = reuse_params\n","  else:\n","    units, dropout, lr, batch_size, loss = params\n","    print(units, dropout, lr, batch_size, loss)\n","  batch_size = 2**batch_size  # use as exponent\n","\n","  print(f'\\nParameter:\\n units:{units} dropout:{dropout} lr:{lr} batch_size:{batch_size} loss:{loss}\\n')\n","\n","  # Define all the parameters\n","  x_train, y_train, x_test, y_test, label_names = preprocessCIFAR10_2()\n","\n","  # preprocess data\n","  if model_name == 'resNet50':\n","    x_tr = resnet50.preprocess_input(x_train)\n","    x_te = resnet50.preprocess_input(x_test)\n","  elif model_name == 'inceptionv3':\n","    x_tr = inception_v3.preprocess_input(x_train)\n","    x_te = inception_v3.preprocess_input(x_test)\n","  elif model_name == 'vgg16':\n","    x_tr = vgg16.preprocess_input(x_train)\n","    x_te = vgg16.preprocess_input(x_test)\n","  else:\n","    raise ValueError('unknown model_name (typo?):', model_name)\n","\n","  boundry = int(len(x_tr)*0.9)\n","  x_val = x_tr[boundry:]\n","  x_tr = x_tr[:boundry]\n","  y_val = y_train[boundry:]\n","  y_tr = y_train[:boundry]\n","\n","  # create model\n","  model, base_model, new_layers = create_model(model_name, nlayers, units, dropout)\n","\n","  # Freezing the model\n","  base_model.trainable = False\n","  new_layers.trainable = True\n","\n","  # train\n","  es = EarlyStopping(patience=patience,  restore_best_weights=True, monitor=\"val_accuracy\", baseline=0.5)\n","  model.compile(optimizer=Adam(learning_rate=lr, amsgrad=True), loss=loss(), metrics=['accuracy'])\n","  history = model.fit(x_tr, y_tr, batch_size=batch_size, epochs=epochs1, validation_data=(x_val, y_val), callbacks=[es])\n","  t_end1 = perf_counter()\n","\n","  # save & evaluate\n","  if eval:\n","    model.save(f'{model_name}_transformed.h5')\n","    print(f\"\\nResults for the training of top layer of {model_name}:\\n\")\n","    print(f'time: {t_end1 - t_start:.4f}secs')\n","    model.summary()\n","    print_report(model, x_te, y_test, label_names)\n","    print_learning_curves(history)\n","\n","  # Freeze all the layers before the `fine_tune_at` layer\n","  new_layers.trainable = True\n","  base_model.trainable = True\n","  fine_tune_at = len(base_model.layers) // 2\n","  for layer in base_model.layers[:fine_tune_at]:\n","    layer.trainable = False\n","\n","  # train\n","  model.compile(optimizer=Adam(learning_rate=lr/10, amsgrad=True), loss=loss(), metrics=['accuracy'])\n","  history = model.fit(x_tr, y_tr, batch_size=batch_size, epochs=epochs2, validation_data=(x_val, y_val), callbacks=[es])\n","  t_end2 = perf_counter()\n","\n","  # save & evaluate\n","  if eval:\n","    model.save(f'{model_name}_tuned.h5')\n","    print(f\"\\nResults for the fine tunig of top layer of {model_name}:\\n\")\n","    print(f'time: {t_end2 - t_start:.4f}secs')\n","    print_report(model, x_te, y_test, label_names)\n","    print_learning_curves(history)\n","\n","\n","  # result\n","  acc = max(history.history['val_accuracy'])\n","  print(f'Final accuracy for model: {acc}\\n')\n","    \n","  return -1 * acc # score for minimization"],"metadata":{"id":"3XgRaRKL6H_C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def optimize_params(model_name, nlayers, reuse_params=None, opti_iter=10):\n","\n","  space = [\n","    Integer(16, 512, prior='log-uniform', base=2,  name='units'),\n","    Real(0.0, 0.6,   prior='uniform',     base=10, name='dropout_rate'),\n","    Real(1e-5, 1e-3, prior='log-uniform', base=10, name='learning_rate'),  # reuse\n","    Integer(4, 6,    prior='uniform',     base=10, name='batch_size'),     # reuse\n","    Categorical([CategoricalCrossentropy, KLDivergence], name='loss')      # reuse\n","  ]\n","\n","  if reuse_params:  # remove lr, bs, loss from space\n","    space = space[:2]\n","\n","  fct = lambda params: train_model(params, model_name, nlayers, reuse_params)\n","  cp_fname = f\"{gdrive_dir}bo_checkpoint_{model_name}_layers{nlayers}.pkl\"\n","\n","  # file already exists - optimization has been interrupted\n","  if os.path.exists(cp_fname):\n","    res = skopt.load(cp_fname)\n","    x0 = res.x_iters\n","    y0 = res.func_vals\n","    init_points=0\n","    print(\"imported parameters\", x0)\n","    print(\"imported values\", y0)\n","    opti_iter = opti_iter - min(opti_iter,len(y0))\n","  else:\n","    x0=None\n","    y0=None\n","    init_points=4\n","\n","  res = gp_minimize(fct,                          # the function to minimize\n","                    space,                        # the bounds on each dimension of x\n","                    n_initial_points=init_points, # initial random points\n","                    x0=x0,                        # Initial input points\n","                    y0=y0,                        # evaluation of x0\n","                    n_calls=opti_iter,            # the number of evaluations of f\n","                    callback=[CheckpointSaver(cp_fname, compress=9)],\n","                    random_state=420)             # the random seed\n","\n","  # evaluate\n","  print(f\"\\nResults for the Baysian Optimization for {model_name}:\")\n","  print('best parameters: ', res.x)\n","  print('best accuracy: ', -res.fun)\n","  plot_convergence(res)\n","  plt.plot()\n","\n","  return res"],"metadata":{"id":"tbqXkNYtVRnx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # optimize layers=1\n","# resNet50_res = optimize_params(\"resNet50\", nlayers=1, opti_iter=20)\n","# units_resNet50 = resNet50_res.x[0]\n","# acc = -resNet50_res.fun\n","\n","# # optimize next layers\n","# for layer in range(2, 5):\n","#   print('\\n_________________________\\nNumber of layers:', layer)\n","#   new_resNet50_res = optimize_params(\"resNet50\", nlayers=layer, units=units_resNet50)\n","#   new_acc = -new_resNet50_res.fun\n","\n","#   if new_acc < acc: break\n","#   acc = new_acc\n","#   resNet50_res = new_resNet50_res\n","\n","# final_nlayers = layer-1\n","# print('\\nFinal number of layers:',final_nlayers)\n","\n","# train_model(resNet50_res.x[:2], \"resNet50\", nlayers=final_nlayers, units=units_resNet50, eval=True)\n","# final_resNet50 = load_model(f'resNet50_tuned.h5')\n","# final_resNet50.save(f'{gdrive_dir}final_resNet50.h5')"],"metadata":{"id":"WjR7ZuMjh9Sd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# optimize layers=1\n","inceptionv3_res = optimize_params(\"inceptionv3\", nlayers=1, opti_iter=35)\n","lr_v3, bs_v3, loss_v3 = inceptionv3_res.x[2:]  # reuse\n","acc = -inceptionv3_res.fun\n","\n","# optimize next layers\n","for layer in range(2, 5):\n","  print('\\n_________________________\\nNumber of layers:', layer)\n","  new_inceptionv3_res = optimize_params(\"inceptionv3\", nlayers=layer, \n","                                     reuse_params=[lr_v3, bs_v3, loss_v3], \n","                                     opti_iter=15)\n","  new_acc = -inceptionv3_res.fun\n","  print(new_acc, acc)\n","  if new_acc < acc: break\n","  acc = new_acc\n","  inceptionv3_res = new_inceptionv3_res\n","\n","final_nlayers = layer-1\n","print('\\nFinal number of layers:',final_nlayers)\n","\n","# evaluate & save in drive\n","train_model(inceptionv3_res.x, \"inceptionv3\", nlayers=final_nlayers, reuse_params=[lr_v3, bs_v3, loss_v3], eval=True)\n","final_inceptionv3 = load_model(f'inceptionv3_tuned.h5')\n","final_inceptionv3.save(f'{gdrive_dir}final_inceptionv3.h5')"],"metadata":{"id":"Ix8F302dXjCm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # optimize layers=1\n","# vgg16_res = optimize_params(\"vgg16\", nlayers=1, opti_iter=20)\n","# units_vgg16 = vgg16_res.x[0]\n","# acc = -vgg16_res.fun\n","\n","# # optimize next layers\n","# for layer in range(2, 5):\n","#   print('\\n_________________________\\nNumber of layers:', layer)\n","#   new_vgg16_res = optimize_params(\"vgg16\", nlayers=layer, units=units_vgg16)\n","#   new_acc = -vgg16_res.fun\n","\n","#   if new_acc < acc: break\n","#   acc = new_acc\n","#   vgg16_res = new_vgg16_res\n","\n","# final_nlayers = layer-1\n","# print('\\nFinal number of layers:',final_nlayers)\n","\n","# #save in g-drive\n","# train_model(vgg16_res.x, \"vgg16\", nlayers=final_nlayers, units=units_vgg16, eval=True)\n","# final_vgg16 = load_model(f'vgg16_tuned.h5')\n","# final_vgg16.save(f'{gdrive_dir}final_vgg16.h5')"],"metadata":{"id":"aXPI0abgXk4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # optimize with one layers\n","# resNet50_res = optimize_params(\"resNet50\", nlayers=1, opti_iter=35)\n","# lr_resNet50, bs_resNet50, loss_resNet50 = resNet50_res.x[2:]  # reuse\n","# acc = -resNet50_res.fun\n","\n","# # optimize next layers\n","# for layer in range(2, 5):\n","#   print('\\n_________________________\\nNumber of layers:', layer)\n","#   new_resNet50_res = optimize_params(\"resNet50\", \n","#                                      nlayers=layer, \n","#                                      reuse_params=[lr_resNet50, bs_resNet50, loss_resNet50], \n","#                                      opti_iter=15)\n","#   new_acc = -new_resNet50_res.fun\n","\n","#   if new_acc < acc: break\n","#   acc = new_acc\n","#   resNet50_res = new_resNet50_res\n","\n","# final_nlayers = layer-1\n","# print('\\nFinal number of layers:', final_nlayers)\n","\n","# # evaluate\n","# # train_model(params, model_name, nlayers, reuse_params=None, eval=False):\n","# train_model(resNet50_res.x[:2], \n","#             \"resNet50\", \n","#             nlayers=final_nlayers, \n","#             reuse_params=[lr_resNet50, bs_resNet50, loss_resNet50], \n","#             eval=True)\n","# final_resNet50 = load_model(f'resNet50_tuned.h5')\n","# final_resNet50.save(f'{gdrive_dir}final_resNet50.h5')"],"metadata":{"id":"5zDHafed_F02"},"execution_count":null,"outputs":[]}]}